{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference with the Spherical Fisher Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores using the [Spherical Fisher Divergence (SFD)](https://arxiv.org/abs/1510.00861) for posterior variational inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we'll use data drawn from a Gaussian Mixture Model (GMM).  Here is a function to draw samples from a GMM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert a.shape[0] == b.shape[0]\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 300 # number of datapoints \n",
    "input_d = 2\n",
    "\n",
    "# Define mixture model\n",
    "pi = np.array([.35, .65])\n",
    "mu_s = [np.array([-5., -5.]), np.array([5., 5.])]\n",
    "cov_s = [np.array([[1., 0.], [0., 1.]]), np.array([[1., 0.], [0., 1.]])]\n",
    "\n",
    "# draw_samples\n",
    "X_train, y_train = draw_samples(pi, mu_s, cov_s, N)\n",
    "\n",
    "# shuffle \n",
    "X_train, y_train = shuffle_in_unison_inplace(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Logistic Regression\n",
    "Our first task will be to classify the component from which each datapoint was drawn.  Let's now write our first TensorFlow code.  The first thing to do is make *symbolic variables* for the input features and the labels.  The data are put in what are called *place holders*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Make symbolic variables\n",
    "X = tf.placeholder(\"float\", [None, input_d]) # samples to discriminate\n",
    "Y = tf.placeholder(\"float\", [None, 1]) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create functions to initialize and run the regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_regression_model(in_size, std=.1):\n",
    "    return {'w':tf.Variable(tf.random_normal([in_size, 1], stddev=std)), 'b':tf.Variable(tf.zeros([1,]))}\n",
    "\n",
    "def linear_regressor(X, params):\n",
    "    return tf.matmul(X, params['w']) + params['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use those functions and create a symbolic cost..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initalize the model parameters\n",
    "model_params = init_regression_model(input_d)\n",
    "\n",
    "# define the model's output\n",
    "linear_model_out = linear_regressor(X, model_params)\n",
    "\n",
    "# define the cost function\n",
    "ce_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(linear_model_out, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set training params\n",
    "n_epochs = 20\n",
    "learning_rate = .1\n",
    "\n",
    "# get the training operator\n",
    "train_model = tf.train.GradientDescentOptimizer(learning_rate).minimize(ce_cost, var_list=[model_params['w'], model_params['b']])\n",
    "\n",
    "final_params = None\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        # perform update\n",
    "        _, loss = session.run([train_model, ce_cost], feed_dict={X: X_train, Y: y_train})\n",
    "        print \"Epoch %d.  Cross-entropy error: %.3f\" %(epoch_idx, loss)\n",
    "        \n",
    "    # save the final params\n",
    "    # NOTICE: this needs to be done within the session!\n",
    "    final_params = {'w':session.run(model_params['w']), 'b':session.run(model_params['b'])}\n",
    "    \n",
    "print final_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
